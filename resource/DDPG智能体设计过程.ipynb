{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DDPG智能体设计过程.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"RBmcxQQMQln7","colab_type":"text"},"source":["# 飞行器模型搭建"]},{"cell_type":"markdown","metadata":{"id":"CG2XPskrOXH3","colab_type":"text"},"source":["## 回放缓冲区\n","大多数现代强化学习算法都使用一个回放存储器或缓冲区来**存储**和**回调**经验元组。"]},{"cell_type":"code","metadata":{"id":"Iq-Nzja5OMBJ","colab_type":"code","colab":{}},"source":["import random\n","from collections import namedtuple, deque\n","\n","class ReplayBuffer:\n","    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n","\n","    def __init__(self, buffer_size, batch_size):\n","        \"\"\"Initialize a ReplayBuffer object.\n","        Params\n","        ======\n","            buffer_size: maximum size of buffer\n","            batch_size: size of each training batch\n","        \"\"\"\n","        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n","        self.batch_size = batch_size\n","        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n","\n","    def add(self, state, action, reward, next_state, done):\n","        \"\"\"Add a new experience to memory.\"\"\"\n","        e = self.experience(state, action, reward, next_state, done)\n","        self.memory.append(e)\n","\n","    def sample(self, batch_size=64):\n","        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n","        return random.sample(self.memory, k=self.batch_size)\n","\n","    def __len__(self):\n","        \"\"\"Return the current size of internal memory.\"\"\"\n","        return len(self.memory)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aLTWeYshOeu2","colab_type":"text"},"source":["## 深度确定性策略梯度 (DDPG)\n","你可以使用很多种不同的算法来设计智能体，只要它适合连续状态和动作空间即可。一种热门方法是深度确定性策略梯度，简称 DDPG。它实际上是一种行动者-评论者方法，但是关键在于底层的策略函数本身确定性函数，从外部添加了一些噪点，以便采取的动作具有理想的随机性。\n","\n","我们来实现原始论文中给出的算法：\n","\n","Lillicrap, Timothy P等，2015. [深度强化学习连续控制](https://arxiv.org/pdf/1509.02971.pdf).\n","\n","可以使用最现代的深度学习库（例如 Keras 或 TensorFlow）实现该算法的两大组件 - 行动者和评论者网络。\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JApHY28GOsov","colab_type":"text"},"source":["### DDPG：行动者（策略）模型\n","以下是使用 Keras 定义的一个非常简单的行动者模型。"]},{"cell_type":"code","metadata":{"id":"Qsqp--9oOwGW","colab_type":"code","colab":{}},"source":["from keras import layers, models, optimizers\n","from keras import backend as K\n","\n","class Actor:\n","    \"\"\"Actor (Policy) Model.\"\"\"\n","\n","    def __init__(self, state_size, action_size, action_low, action_high):\n","        \"\"\"Initialize parameters and build model.\n","\n","        Params\n","        ======\n","            state_size (int): Dimension of each state\n","            action_size (int): Dimension of each action\n","            action_low (array): Min value of each action dimension\n","            action_high (array): Max value of each action dimension\n","        \"\"\"\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.action_low = action_low\n","        self.action_high = action_high\n","        self.action_range = self.action_high - self.action_low\n","\n","        # Initialize any other variables here\n","\n","        self.build_model()\n","\n","    def build_model(self):\n","        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n","        # Define input layer (states)\n","        states = layers.Input(shape=(self.state_size,), name='states')\n","\n","        # Add hidden layers\n","        net = layers.Dense(units=32, activation='relu')(states)\n","        net = layers.Dense(units=64, activation='relu')(net)\n","        net = layers.Dense(units=32, activation='relu')(net)\n","\n","        # Try different layer sizes, activations, add batch normalization, regularizers, etc.\n","\n","        # Add final output layer with sigmoid activation\n","        raw_actions = layers.Dense(units=self.action_size, activation='sigmoid',\n","            name='raw_actions')(net)\n","\n","        # Scale [0, 1] output for each action dimension to proper range\n","        actions = layers.Lambda(lambda x: (x * self.action_range) + self.action_low,\n","            name='actions')(raw_actions)\n","\n","        # Create Keras model\n","        self.model = models.Model(inputs=states, outputs=actions)\n","\n","        # Define loss function using action value (Q value) gradients\n","        action_gradients = layers.Input(shape=(self.action_size,))\n","        loss = K.mean(-action_gradients * actions)\n","\n","        # Incorporate any additional losses here (e.g. from regularizers)\n","\n","        # Define optimizer and training function\n","        optimizer = optimizers.Adam()\n","        updates_op = optimizer.get_updates(params=self.model.trainable_weights, loss=loss)\n","        self.train_fn = K.function(\n","            inputs=[self.model.input, action_gradients, K.learning_phase()],\n","            outputs=[],\n","            updates=updates_op)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9z7GLLQJO-ZO","colab_type":"text"},"source":["注意，输出层生成的原始动作位于[0.0, 1.0]范围内（使用 sigmoid 激活函数）。因此，我们添加另一个层级，该层级会针对每个动作维度将每个输出缩放到期望的范围。这样会针对任何给定状态向量生成确定性动作。稍后将向此动作添加噪点，以生成某个探索性行为。\n","\n","另一个需要注意的是损失函数如何使用动作值（Q 值）梯度进行定义：\n","\n","```\n","# Define loss function using action value (Q value) gradients\n","action_gradients = layers.Input(shape=(self.action_size,))\n","loss = K.mean(-action_gradients * actions)\n","```\n","\n","这些梯度需要使用评论者模型计算，并在训练时提供梯度。因此指定为在训练函数中使用的“输入”的一部分：\n","\n","\n","```\n","self.train_fn = K.function(\n","   inputs=[self.model.input, action_gradients, K.learning_phase()],\n","    outputs=[],\n","    updates=updates_op)\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6M-NC8UePP8l","colab_type":"text"},"source":["### DDPG: 评论者模型\n","相应的评论者模型可以如下所示："]},{"cell_type":"code","metadata":{"id":"m0v8hU1jPbKH","colab_type":"code","colab":{}},"source":["class Critic:\n","    \"\"\"Critic (Value) Model.\"\"\"\n","\n","    def __init__(self, state_size, action_size):\n","        \"\"\"Initialize parameters and build model.\n","\n","        Params\n","        ======\n","            state_size (int): Dimension of each state\n","            action_size (int): Dimension of each action\n","        \"\"\"\n","        self.state_size = state_size\n","        self.action_size = action_size\n","\n","        # Initialize any other variables here\n","\n","        self.build_model()\n","\n","    def build_model(self):\n","        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n","        # Define input layers\n","        states = layers.Input(shape=(self.state_size,), name='states')\n","        actions = layers.Input(shape=(self.action_size,), name='actions')\n","\n","        # Add hidden layer(s) for state pathway\n","        net_states = layers.Dense(units=32, activation='relu')(states)\n","        net_states = layers.Dense(units=64, activation='relu')(net_states)\n","\n","        # Add hidden layer(s) for action pathway\n","        net_actions = layers.Dense(units=32, activation='relu')(actions)\n","        net_actions = layers.Dense(units=64, activation='relu')(net_actions)\n","\n","        # Try different layer sizes, activations, add batch normalization, regularizers, etc.\n","\n","        # Combine state and action pathways\n","        net = layers.Add()([net_states, net_actions])\n","        net = layers.Activation('relu')(net)\n","\n","        # Add more layers to the combined network if needed\n","\n","        # Add final output layer to prduce action values (Q values)\n","        Q_values = layers.Dense(units=1, name='q_values')(net)\n","\n","        # Create Keras model\n","        self.model = models.Model(inputs=[states, actions], outputs=Q_values)\n","\n","        # Define optimizer and compile model for training with built-in loss function\n","        optimizer = optimizers.Adam()\n","        self.model.compile(optimizer=optimizer, loss='mse')\n","\n","        # Compute action gradients (derivative of Q values w.r.t. to actions)\n","        action_gradients = K.gradients(Q_values, actions)\n","\n","        # Define an additional function to fetch action gradients (to be used by actor model)\n","        self.get_action_gradients = K.function(\n","            inputs=[*self.model.input, K.learning_phase()],\n","            outputs=action_gradients)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OsmZAeCePeMt","colab_type":"text"},"source":["它在某些方面比行动者模型简单，但是需要注意几点。首先，行动者模型旨在将状态映射到动作，而评论者模型需要将（状态、动作）对映射到它们的 Q 值。这一点体现在了输入层中。\n","\n","\n","\n","```\n","# Define input layers\n","states = layers.Input(shape=(self.state_size,), name='states')\n","actions = layers.Input(shape=(self.action_size,), name='actions')\n","\n","```\n","这两个层级首先可以通过单独的“路径”（迷你子网络）处理，但是最终需要结合到一起。例如，可以通过使用 Keras 中的 Add 层级类型来实现请参阅[合并层级](https://keras.io/layers/merge/)):\n","\n","\n","\n","\n","```\n","# Combine state and action pathways\n","net = layers.Add()([net_states, net_actions])\n","```\n","\n","\n","该模型的最终输出是任何给定（状态、动作）对的 Q 值。但是，我们还需要计算此 Q 值相对于相应动作向量的梯度，以用于训练行动者模型。这一步需要明确执行，并且需要定义一个单独的函数来访问这些梯度：\n","\n","```\n","# Compute action gradients (derivative of Q values w.r.t. to actions)\n","action_gradients = K.gradients(Q_values, actions)\n","\n","# Define an additional function to fetch action gradients (to be used by actor model)\n","self.get_action_gradients = K.function(\n","    inputs=[*self.model.input, K.learning_phase()],\n","    outputs=action_gradients)\n","```"]},{"cell_type":"markdown","metadata":{"id":"a1cM1ASPP1Hu","colab_type":"text"},"source":["### DDPG: 智能体\n","现在我们可以将行动者模型和策略模型放到一起，构建 DDPG 智能体了。注意，我们需要每个模型的两个副本，一个本地副本，一个目标副本。该技巧来自深度 Q 学习中的“固定 Q 目标”，用于拆分被更新的参数和生成目标值的参数。\n","\n","以下是智能体类的纲要："]},{"cell_type":"code","metadata":{"id":"OVxJUNx1P2tc","colab_type":"code","colab":{}},"source":["class DDPG():\n","    \"\"\"Reinforcement Learning agent that learns using DDPG.\"\"\"\n","    def __init__(self, task):\n","        self.task = task\n","        self.state_size = task.state_size\n","        self.action_size = task.action_size\n","        self.action_low = task.action_low\n","        self.action_high = task.action_high\n","\n","        # Actor (Policy) Model\n","        self.actor_local = Actor(self.state_size, self.action_size, self.action_low, self.action_high)\n","        self.actor_target = Actor(self.state_size, self.action_size, self.action_low, self.action_high)\n","\n","        # Critic (Value) Model\n","        self.critic_local = Critic(self.state_size, self.action_size)\n","        self.critic_target = Critic(self.state_size, self.action_size)\n","\n","        # Initialize target model parameters with local model parameters\n","        self.critic_target.model.set_weights(self.critic_local.model.get_weights())\n","        self.actor_target.model.set_weights(self.actor_local.model.get_weights())\n","\n","        # Noise process\n","        self.exploration_mu = 0\n","        self.exploration_theta = 0.15\n","        self.exploration_sigma = 0.2\n","        self.noise = OUNoise(self.action_size, self.exploration_mu, self.exploration_theta, self.exploration_sigma)\n","\n","        # Replay memory\n","        self.buffer_size = 100000\n","        self.batch_size = 64\n","        self.memory = ReplayBuffer(self.buffer_size, self.batch_size)\n","\n","        # Algorithm parameters\n","        self.gamma = 0.99  # discount factor\n","        self.tau = 0.01  # for soft update of target parameters\n","\n","    def reset_episode(self):\n","        self.noise.reset()\n","        state = self.task.reset()\n","        self.last_state = state\n","        return state\n","\n","    def step(self, action, reward, next_state, done):\n","         # Save experience / reward\n","        self.memory.add(self.last_state, action, reward, next_state, done)\n","\n","        # Learn, if enough samples are available in memory\n","        if len(self.memory) > self.batch_size:\n","            experiences = self.memory.sample()\n","            self.learn(experiences)\n","\n","        # Roll over last state and action\n","        self.last_state = next_state\n","\n","    def act(self, states):\n","        \"\"\"Returns actions for given state(s) as per current policy.\"\"\"\n","        state = np.reshape(states, [-1, self.state_size])\n","        action = self.actor_local.model.predict(state)[0]\n","        return list(action + self.noise.sample())  # add some noise for exploration\n","\n","    def learn(self, experiences):\n","        \"\"\"Update policy and value parameters using given batch of experience tuples.\"\"\"\n","        # Convert experience tuples to separate arrays for each element (states, actions, rewards, etc.)\n","        states = np.vstack([e.state for e in experiences if e is not None])\n","        actions = np.array([e.action for e in experiences if e is not None]).astype(np.float32).reshape(-1, self.action_size)\n","        rewards = np.array([e.reward for e in experiences if e is not None]).astype(np.float32).reshape(-1, 1)\n","        dones = np.array([e.done for e in experiences if e is not None]).astype(np.uint8).reshape(-1, 1)\n","        next_states = np.vstack([e.next_state for e in experiences if e is not None])\n","\n","        # Get predicted next-state actions and Q values from target models\n","        #     Q_targets_next = critic_target(next_state, actor_target(next_state))\n","        actions_next = self.actor_target.model.predict_on_batch(next_states)\n","        Q_targets_next = self.critic_target.model.predict_on_batch([next_states, actions_next])\n","\n","        # Compute Q targets for current states and train critic model (local)\n","        Q_targets = rewards + self.gamma * Q_targets_next * (1 - dones)\n","        self.critic_local.model.train_on_batch(x=[states, actions], y=Q_targets)\n","\n","        # Train actor model (local)\n","        action_gradients = np.reshape(self.critic_local.get_action_gradients([states, actions, 0]), (-1, self.action_size))\n","        self.actor_local.train_fn([states, action_gradients, 1])  # custom training function\n","\n","        # Soft-update target models\n","        self.soft_update(self.critic_local.model, self.critic_target.model)\n","        self.soft_update(self.actor_local.model, self.actor_target.model)   \n","\n","    def soft_update(self, local_model, target_model):\n","        \"\"\"Soft update model parameters.\"\"\"\n","        local_weights = np.array(local_model.get_weights())\n","        target_weights = np.array(target_model.get_weights())\n","\n","        assert len(local_weights) == len(target_weights), \"Local and target model parameters must have the same size\"\n","\n","        new_weights = self.tau * local_weights + (1 - self.tau) * target_weights\n","        target_model.set_weights(new_weights)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2wZJDyBrP6Ps","colab_type":"text"},"source":["注意，在用多个经验进行训练后，我们可以将新学习的权重（来自本地模型）复制到目标模型中。但是，单个批次可能会向该流程中引入很多偏差，因此最后进行软更新，由参数 tau 控制。\n","\n","最后还要完成一步，才能使整个项目能正常运行，你需要创建合适的噪点模型。"]},{"cell_type":"markdown","metadata":{"id":"lZ7tabp7P99c","colab_type":"text"},"source":["## Ornstein–Uhlenbeck 噪点\n","我们将使用一个具有所需特性的特定噪点流程，称之为 [Ornstein–Uhlenbeck 流程](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process)。它会根据高斯（正态）分布生成随机样本，但是每个样本都会影响到后续样本，使两个连续样本很接近。因此本质上是马尔可夫流程。\n","\n","为何与我们有关呢？我们可以直接从高斯分布中抽样吧？是的，但是注意，我们希望使用该流程向动作中添加一些噪点，以便促进探索性行为。因为动作就是应用到飞行器上的力和扭矩，我们希望连续动作不要变化太大。否则，我们可能不会飞到任何地方！想象下随机上下左右翻转控制器！\n","\n","除了样本的暂时相关特性外，QU 流程的另一个好处是在一段时间之后，它会逐渐接近指定的均值。在用它生成噪点时，我们可以指定均值为 0，当我们在学习任务上取得进展时，它将能够减少探索行为。\n","\n","以下是一个你可以采用的示例 Ornstein-Uhlenbeck 流程实现。"]},{"cell_type":"code","metadata":{"id":"ee1O3tHwQFj8","colab_type":"code","colab":{}},"source":["class OUNoise:\n","    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n","\n","    def __init__(self, size, mu=None, theta=0.15, sigma=0.3):\n","        \"\"\"Initialize parameters and noise process.\"\"\"\n","        self.size = size\n","        self.mu = mu if mu is not None else np.zeros(self.size) #此处需修改mu\n","        self.theta = theta\n","        self.sigma = sigma\n","        self.state = np.ones(self.size) * self.mu\n","        self.reset()\n","\n","    def reset(self):\n","        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n","        self.state = self.mu\n","\n","    def sample(self):\n","        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n","        x = self.state\n","        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n","        self.state = x + dx\n","        return self.state"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pDZMa8ghQPgE","colab_type":"text"},"source":["### 我的飞行器需要训练多久？\n","根据随机的初始参数，有时你的飞行器会在初始的20-50个迭代循环里习得你的任务，但有时你会可能需要这个训练次数达到500-1000次。此外，你的飞行器也可能陷入局部最优解，而无法达到更优的学习效果。因此，你的飞行器可能需要很长时间来完成你的任务，这个主要依赖于你选择的学习率learning rate。\n","\n","如果你发现你的飞行器学习了1500-2000次仍然没有显著进展，你可以先让你的飞行器先试试简单的连续动作问题，如 [Mountain Car](https://gym.openai.com/envs/MountainCarContinuous-v0/) 或者 [Pendulum](https://gym.openai.com/envs/Pendulum-v0/) 。当然，你的模型大小会随着你状态/行动空间的不同定义而改变，但是至少可以保障你的算法实现。\n","\n","其次，调整飞行器的状态以及/或者动作空间定义，来保障你的飞行器拥有足够的学习参数（但不要过多！）最后（这是许多最多迭代的部分），尝试调整奖励函数来让你的飞行器得到一个更具体的学习目的。调整矩阵里的**权重**，增加额外的奖励/惩罚，尝试一个**非线性映射矩阵**。同时，你可能也需要将**奖惩的值标准化**，如从-1.0到1.0。这样的转换将避免梯度爆炸造成的不稳定性。\n","\n","### 我的飞行器的确在学习，但是即便是过了很久，依然没有达到我的要求。我该怎么做？\n","想要让一个强化学习智能体学习你_实际_想要它学的，是十分困难的，也十分费时。你的飞行器只会根据你定义的奖励函数来习得最优的策略，但是你的奖励函数可能没有包含你期待的所有方面，比如飞行任务等。如果你没有预设你的飞行器做一些转体，只是关心它飞多高，那它就很可能会旋转上升！\n","\n","有时，你的算法可能没有与环境交互好，如，可能部分环境需要额外的探索及噪音，而部分环境则可以减少噪音。你可以试试改变一些变量，来看会不会优化结果。但是即使你已经有了一个很优秀的算法，你都可能需要几天或几周来习得一个复杂的任务！\n","\n","这就是为什么我们希望你绘制一个奖励曲线。只要每次迭代的平均奖励是总体上升的（即便存在一些噪音），就可以了。飞行最后的行为展示不需要非常完美。\n","\n","### 我可以使用一个深度Q网络来解决问题吗？\n","当然！但是...记得一个深度Q网络（DQN）意味着解决一个离散动作空间问题，然而我们的飞行器控制问题是一个连续动作域问题。因此我们并不建议一个离散空间算法，但是你可以仍旧使用这个DQN来映射最终的输出结果至一个适当的连续动作空间。尽管这个过程可能让神经网络难以理解动作之间的互相关联，但是它可能也会简化这个训练算法。"]}]}